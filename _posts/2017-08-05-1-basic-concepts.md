---
layout: post
title:  "数据挖掘基本概念"
date:   2017-08-05 15:14:54
categories: 大数据
tags: 数据挖掘
excerpt: 本笔记来自于阅读《大数据-互联网大规模数据挖掘与分布式处理》第1章。
mathjax: true
---

# 数据挖掘基本概念 #
---
##  什么是数据挖掘 ##

数据挖掘，又称数据中的知识发现，从大量的数据中挖掘有趣模式和知识的过程，书中给出的定义：是数据“模型”的发现过程。然后根据“模型”的不同含义给出了建模的几个重要方向。

- 统计建模：统计模型`Staticical model`的建模过程，也就是可见数据遵循的总体分布。思路是初步判定数据可能符合某种分布，然后利用公示计算参数值。

- 机器学习： 机器学习本身专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。 一些数据挖掘方法中会用到机器学习算法，比如贝叶斯网络、支持向量机，决策树，隐马尔科夫模型等。**注意**：机器学习典型主要适用在对数据挖掘的目标几乎一无所知的场景，而当挖掘目标明确的场景下无任何优势。书中举例：简历搜索。

- 建模的计算方法：大部分数据建模方法可以分为两类：
	* 数据汇总：`PageRank`（网页重要性）和聚类（病例传播的例子）。
	* 从数据中抽取出最突出的特征来代替数据，并忽略其他内容。特征抽取有频繁项、相似项等（为顾客推荐感兴趣商品）。

## 数据挖掘的统计限制 ##
数据挖掘会不会涉及隐私和安全问题？ TIA(Total Information Awareness)计划未被美国国会通过这里例子说明了一些问题。

邦弗朗尼原理`Bonferroni’s principle`，大概意思是要避免统计假象，书中示例：通过对某些数据统计，可能推断出每年要发生很多起恐怖袭击，而这本身是不现实的。所以不要指望通过统计来「臆造」一些稀有的事情。 **数据挖掘不能过度使用！**
至少有2次 同一天住在同一宾馆。假设：
1. 追踪人数：10亿（$10^9$）
2. 每个人每100天当中会有一天去宾馆；
3. 一个宾馆最多容纳100个人。因此，100 000个宾馆已足够容纳10亿人中的1%在某个给定的日子入住宾馆；
4. 对1000天的宾馆入住记录进行核查。

给定任意两个人都决定去宾馆的概率为0.0001，入住同一宾馆的概率为$0.0001/10^5$，那么，在任意给定的两个不同日子，两人入住同一宾馆的概率为$10^{-18}$。

为简化运算，对于较大n，$\binom{n}{k}$约等于$n^2/2$。$10^9$人组队个数$\binom{10^9}{2}=5 \times 10^{17}$，而1000天内任意两天的组合数$\binom{1000}{2}=5 \times 10^5$。疑似坏人的期望为 $5 \times 10^{17} \times 5 \times 10^5 \times 10^{-18} = 250000$，即月25万对人员看上去像坏人。

但实际上，真正坏人的数量明显少人多，那么通过调查25万对人员来确定真正的坏人，不仅会侵犯50万无辜人员的生活，而且所需工作量巨大，因此，这种做法是不可行的。

## 数据挖掘相关知识 ##
- 用于度量词语重要性的TF.IDF指标

	数据挖掘很多应用涉及根据主题对文档（词语的序列）进行分类。分类的第一步是从文档中找出重要的词语。书上以马球运动为例：

	- 最频繁出现的词语"the"、"and"等其实并不重要。
	- 事实上，描述主题的词语往往相对出现次数较少。
	- 类似"albeit"的词语在文档中出现并不会增加它多次出现的可能性；但是如果文章提到“chukker”，则很可能会提到"first chukker","second chukker"...(反复出现)。

	**TF.IDF** 用于度量给定词语在文档中反复出现程度的指标，其中TF是词项频率`Term Frequency`，IDF是逆文档频率`Inverse Document Frequency`。

	假定有$N$篇文档，$ f_{ij} $ 为词项 $ i $ 在文档 $ j $ 中出现的频率，则$ TF_{ij}$ 定义为：

	$$
	TF_{ij} = \frac{f_{ij}}{max_k f_{ij}}
	$$

	这里对词项$i$中文档$j$中的词项频率$ f_{ij}$  做了归一化处理。

	假设词项$i$在$n$篇文档中出现，那么词项i的IDF定义如下：

	$$
	IDF_i = log_2 \frac{N}{n_i}
	$$

	于是，词项$i$中文档集中的得分被定义为 $ TF_{ij} \times IDF_i $，具有最高TF.IDF得分的词项通常是刻画文档主题的最佳词项。

	举例来说：有$ 2^{20} $篇文档，词语$w$中其中的$ 2^{10} $篇文档中出现，那么 $IDF_w = log_2(\frac{2^{20}}{2^{10}}) =10$。考虑文档$j$，词语$w$在该文档出现20次，假设是文档出现次数最多的词，那么$ TF_{wj}=1 $，于是$w$中文档$j$中TF.IDF得分为10。假设在文档$k$中，词语$w$出现一次，频率最高的词出现20次，于是$TF_{wk}=1/20$，$w$中文档$k$中s的TF.IDF得分为1/2。

- 哈希函数

	关键在于构造哈希函数和处理冲突。

- 二级存储器及对算法运行时间的影响

	由于磁盘和内存的特性区别，数据一开始在磁盘还是在内存，计算的时间开销相差很大。当处理大规模数据时需要注意数据如何存储，优先内存访问。

- 自然对数的底e

	常数$e=2.7182818$,当$x$趋向于$\infty$ 时，$ (1+\frac{1}x)^x$的极限。一些复杂的表达式可以通过代数公式得到近似值。对于$(1+a)^b$, 当$0<a<<1$时，可以等价为$e^{ab}$。

- 幂定律（power law）

	两个变量在对数空间下呈现出线性的关系，$y = c x^a$，其中$a$是幂，一个通常的取值是-2左右。自然界中存在很多服从幂定律的例子：Web图中节点的度，商品的销量、Web网站的大小、Zipf定律。

- 索引

	索引是在给定一个或多个字段值时进行高效记录存取和检索的一种数据结构。哈希就是构建索引的一种方式。
